{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56a77392",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "CHECKPOINT_PATH = \"../exps/slimpajama/noam/better_scheduler_noam_lr0.0008_bs64x4_seqlen512/iterations=10000_scheduler=cycle_eval_freq=2000_grad_clip=1.0_weight_tying=True_n_head=16_n_layer=10_n_embd=1024_compile=True_save_checkpoint_freq=500_seed=0/ckpt_7000.pt\"\n",
    "\n",
    "CFG_PATH = 'config/final/noam_wide4.yaml'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77ab68fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset 'slimpajama'\n",
      "here\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 177.89M\n",
      "Noam(\n",
      "  (transformer): ModuleDict(\n",
      "    (wte): Embedding(50304, 1024)\n",
      "    (drop): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-9): 10 x LlamaBlock(\n",
      "        (ln_1): RMSNorm()\n",
      "        (attn): LlamaAttention(\n",
      "          (c_attn): Linear(in_features=1024, out_features=3072, bias=False)\n",
      "          (c_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
      "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (rotary_emb): RotaryEmbedding()\n",
      "        )\n",
      "        (ln_2): RMSNorm()\n",
      "        (mlp): LlamaMLP(\n",
      "          (w1): Linear(in_features=1024, out_features=2816, bias=False)\n",
      "          (w2): Linear(in_features=1024, out_features=2816, bias=False)\n",
      "          (c_proj): Linear(in_features=2816, out_features=1024, bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): RMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=1024, out_features=50304, bias=False)\n",
      "  (rotary_emb): RotaryEmbedding()\n",
      ")\n",
      "Load from ../exps/slimpajama/noam/better_scheduler_noam_lr0.0008_bs64x4_seqlen512/iterations=10000_scheduler=cycle_eval_freq=2000_grad_clip=1.0_weight_tying=True_n_head=16_n_layer=10_n_embd=1024_compile=True_save_checkpoint_freq=500_seed=0/ckpt_7000.pt\n"
     ]
    },
    {
     "ename": "UnpicklingError",
     "evalue": "Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL numpy.core.multiarray._reconstruct was not an allowed global by default. Please use `torch.serialization.add_safe_globals([_reconstruct])` to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 292\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmain\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_args  \u001b[38;5;66;03m# Adjust this import path as needed\u001b[39;00m\n\u001b[1;32m    291\u001b[0m args \u001b[38;5;241m=\u001b[39m get_args()\n\u001b[0;32m--> 292\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 66\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     64\u001b[0m last_ckpt_path \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39muse_pretrained\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoad from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlast_ckpt_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 66\u001b[0m checkpoint \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlast_ckpt_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m model_state_dict \u001b[38;5;241m=\u001b[39m {distributed_backend\u001b[38;5;241m.\u001b[39mtranslate_model_parameter_name_for_node(k\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_orig_mod.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))[\u001b[38;5;241m0\u001b[39m]:v \u001b[38;5;28;01mfor\u001b[39;00m k,v \u001b[38;5;129;01min\u001b[39;00m checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# FIXME checkpoints from compiled model have _orig_mod keyword\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# only load the model; ignore scheduler, optimizer, itr\u001b[39;00m\n",
      "File \u001b[0;32m/venv/main/lib/python3.10/site-packages/torch/serialization.py:1359\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1351\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[1;32m   1352\u001b[0m                     opened_zipfile,\n\u001b[1;32m   1353\u001b[0m                     map_location,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1356\u001b[0m                     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args,\n\u001b[1;32m   1357\u001b[0m                 )\n\u001b[1;32m   1358\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1359\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1360\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(\n\u001b[1;32m   1361\u001b[0m             opened_zipfile,\n\u001b[1;32m   1362\u001b[0m             map_location,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1365\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args,\n\u001b[1;32m   1366\u001b[0m         )\n\u001b[1;32m   1367\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n",
      "\u001b[0;31mUnpicklingError\u001b[0m: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL numpy.core.multiarray._reconstruct was not an allowed global by default. Please use `torch.serialization.add_safe_globals([_reconstruct])` to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import inspect\n",
    "import json\n",
    "import copy\n",
    "import argparse\n",
    "import random\n",
    "import wandb\n",
    "\n",
    "import config\n",
    "from models.utils import get_model\n",
    "from data.utils import get_dataset\n",
    "from optim.base import train_base\n",
    "from optim.sofia import SophiaG\n",
    "import distributed\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "\n",
    "from peft import LoKrConfig, LoraConfig, LoHaConfig, get_peft_model\n",
    "\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser(allow_abbrev=False)\n",
    "    parser.add_argument('--config_format', default='base', choices=config.registered_formats())\n",
    "\n",
    "    args, rem_args = parser.parse_known_args()\n",
    "\n",
    "    return config.parse_args_with_format(format=args.config_format, base_parser=parser, args=rem_args, namespace=args)\n",
    "\n",
    "\n",
    "def main(args):\n",
    "\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True # allows us to make sure we're able to use tensorfloat32 during training\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "    distributed_backend = distributed.make_backend_from_args(args)\n",
    "    args = distributed_backend.get_adjusted_args_for_process(args)\n",
    "\n",
    "    args.device = torch.device(args.device)\n",
    "    device_type = \"cuda\" if \"cuda\" in str(args.device) else \"cpu\"\n",
    "    if device_type == \"cuda\":\n",
    "        torch.cuda.set_device(args.device)\n",
    "\n",
    "    torch.manual_seed(args.seed)\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "\n",
    "    print(f\"Loading dataset '{args.dataset}'\")\n",
    "\n",
    "    # data = get_dataset(args) # data is a dict: {'train': train_tokenized, 'val': eval_tokenized}\n",
    "    # if args.data_in_ram:\n",
    "    #     data = {'train': np.array(data['train']), 'val': np.array(data['val'])}\n",
    "\n",
    "    # print(f\"Num training tokens: {len(data['train'])}\")\n",
    "    # print(f\"Num validation tokens: {len(data['val'])}\")\n",
    "\n",
    "    args.device = 'cpu'\n",
    "    args.use_pretrained = CHECKPOINT_PATH\n",
    "\n",
    "    global model\n",
    "    model = get_model(args).to(args.device) # todo: take care of initializing the model if args.use_pretrained != 'none'\n",
    "    print(model)\n",
    "\n",
    "    last_ckpt_path = args.use_pretrained\n",
    "    print(f\"Load from {last_ckpt_path}\")\n",
    "    checkpoint = torch.load(last_ckpt_path, weights_only=True)\n",
    "\n",
    "    model_state_dict = {distributed_backend.translate_model_parameter_name_for_node(k.replace(\"_orig_mod.\", \"\"))[0]:v for k,v in checkpoint['model'].items()}\n",
    "    # FIXME checkpoints from compiled model have _orig_mod keyword\n",
    "\n",
    "    # only load the model; ignore scheduler, optimizer, itr\n",
    "    model.load_state_dict(model_state_dict)\n",
    "\n",
    "\n",
    "    # model = distributed_backend.transform_model(model)\n",
    "\n",
    "    # group_specs = distributed_backend.get_raw_model(model).get_parameter_group_specs()\n",
    "    # param_name_mapping = {p_name: p for p_name, p in model.named_parameters()}\n",
    "    # optimized_params_cnt = 0\n",
    "    # for g in group_specs:\n",
    "    #     params = []\n",
    "    #     for p_name in g[\"params\"]:\n",
    "    #         translated_p_names = distributed_backend.translate_model_parameter_name_for_node(p_name)\n",
    "    #         params += [param_name_mapping[p_name] for p_name in translated_p_names]\n",
    "    #     g[\"params\"] = params\n",
    "    #     optimized_params_cnt += sum([p.numel() for p in g[\"params\"]])\n",
    "    # print(\"number of optimized parameters: %.2fM\" % (optimized_params_cnt/1e6,))\n",
    "    # if args.opt == 'adamw':\n",
    "    #     use_fused = (device_type == 'cuda') and ('fused' in inspect.signature(torch.optim.AdamW).parameters)\n",
    "    #     print(f\"using fused AdamW: {use_fused}\")\n",
    "    #     extra_args = dict(fused=True) if use_fused else dict()\n",
    "    #     opt = torch.optim.AdamW(group_specs, lr=args.lr, betas=(args.beta1, args.beta2),\n",
    "    #                             weight_decay=args.weight_decay, **extra_args)\n",
    "    # elif args.opt == \"sgd\":\n",
    "    #     opt = torch.optim.SGD(group_specs, lr=args.lr, momentum=0.9, weight_decay=args.weight_decay)\n",
    "    # elif args.opt == \"sofiag\":\n",
    "    #     opt = SophiaG(group_specs, lr=args.lr, betas=(args.beta1, args.beta2),\n",
    "    #                             weight_decay=args.weight_decay, rho=args.rho)\n",
    "\n",
    "    # else:\n",
    "    #     raise NotImplementedError(f\"{args.opt} optimizer doesn't exist\")\n",
    "\n",
    "    # if args.scheduler != 'none':\n",
    "    #     if args.scheduler in ['cos', 'linear']:\n",
    "    #         scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer=opt, max_lr=args.lr, total_steps=args.iterations,\n",
    "    #                                                         pct_start=args.warmup_percent, anneal_strategy=args.scheduler,\n",
    "    #                                                         cycle_momentum=False, div_factor=1e2, final_div_factor=.1)\n",
    "    #     elif args.scheduler == 'cycle':\n",
    "    #         scheduler = OneCycleLR(\n",
    "    #             optimizer=opt,\n",
    "    #             max_lr=1e-3,                    # peak learning rate\n",
    "    #             total_steps=10000,             # total number of training steps\n",
    "    #             pct_start=0.3,                 # % of total steps to warm up\n",
    "    #             anneal_strategy='cos',         # or 'linear'\n",
    "    #             div_factor=25,                 # initial_lr = max_lr / div_factor\n",
    "    #             final_div_factor=1e4,          # final_lr = max_lr / final_div_factor\n",
    "    #             three_phase=False,             # typical 2-phase cycle\n",
    "    #             verbose=False\n",
    "    #         )\n",
    "    #     else:\n",
    "    #         raise NotImplementedError(f\"Unknown scheduler type: {args.scheduler}.\")\n",
    "    # else:\n",
    "    #     scheduler = None\n",
    "\n",
    "    # args.world_size = distributed_backend.get_world_size()\n",
    "    # exp_name = args.exp_name\n",
    "    # if distributed_backend.is_master_process() and args.wandb:\n",
    "    #     params_copy = copy.deepcopy(vars(args))\n",
    "    #     del params_copy['device']\n",
    "    #     wandb.init(project=args.wandb_project, name=exp_name, config=params_copy, group=args.wandb_group)\n",
    "\n",
    "    # ckpt_path = os.path.join(args.results_base_folder, args.dataset, args.model, exp_name)\n",
    "    # if not os.path.exists(ckpt_path):\n",
    "    #     if distributed_backend.is_master_process():\n",
    "    #         os.makedirs(ckpt_path)\n",
    "    #     distributed_backend.sync()\n",
    "    # elif os.path.isfile(os.path.join(ckpt_path, \"summary.json\")): # the experiment was already completed\n",
    "    #     print(f\"Already found experiment '{ckpt_path}'.\\nSkipping.\")\n",
    "    #     sys.exit(0)\n",
    "\n",
    "    # itr = 0\n",
    "    # rng_state_dict = None\n",
    "\n",
    "    # if args.use_pretrained is not None and 'ckpt_' in args.use_pretrained:\n",
    "    #     last_ckpt_path = args.use_pretrained\n",
    "    #     print(f\"Resuming from {last_ckpt_path}\")\n",
    "    #     # checkpoint = torch.load(os.path.join(ckpt_path, last_ckpt_path))\n",
    "    #     checkpoint = torch.load(last_ckpt_path, weights_only=True)\n",
    "    #     print(checkpoint)\n",
    "\n",
    "    #     model_state_dict = {distributed_backend.translate_model_parameter_name_for_node(k.replace(\"_orig_mod.\", \"\"))[0]:v for k,v in checkpoint['model'].items()}\n",
    "    #     # FIXME checkpoints from compiled model have _orig_mod keyword\n",
    "\n",
    "    #     optimizer_state_dict = checkpoint['optimizer']\n",
    "    #     rng_state_dict = {\n",
    "    #         module: checkpoint[module] for module in [\n",
    "    #             \"cpu_rng_state\",\n",
    "    #             \"gpu_rng_state\",\n",
    "    #             \"numpy_rng_state\",\n",
    "    #             \"py_rng_state\",\n",
    "    #             \"train_sampler_state\"\n",
    "    #         ]\n",
    "    #     }\n",
    "\n",
    "    #     model.load_state_dict(model_state_dict)\n",
    "    #     opt.load_state_dict(optimizer_state_dict)\n",
    "    #     itr = checkpoint['itr']\n",
    "    #     if scheduler is not None:\n",
    "    #         scheduler_state_dict = checkpoint['scheduler']\n",
    "    #         scheduler.load_state_dict(scheduler_state_dict)\n",
    "\n",
    "    # elif args.use_pretrained is not None:\n",
    "    #     last_ckpt_path = args.use_pretrained\n",
    "    #     print(f\"Load from {last_ckpt_path}\")\n",
    "    #     checkpoint = torch.load(last_ckpt_path, weights_only=True)\n",
    "\n",
    "    #     model_state_dict = {distributed_backend.translate_model_parameter_name_for_node(k.replace(\"_orig_mod.\", \"\"))[0]:v for k,v in checkpoint['model'].items()}\n",
    "    #     # FIXME checkpoints from compiled model have _orig_mod keyword\n",
    "\n",
    "    #     # only load the model; ignore scheduler, optimizer, itr\n",
    "    #     model.load_state_dict(model_state_dict)\n",
    "\n",
    "    # if args.model in ['base', 'llama2', 'noam']: # all train functions have the same interface\n",
    "    #     train = train_base\n",
    "    # else:\n",
    "    #     raise NotImplementedError(f\"No training method implemented for model type '{args.model}'.\")\n",
    "\n",
    "    # # Apply LoRA\n",
    "    # \"\"\"\n",
    "    # Noam(\n",
    "    #   (transformer): ModuleDict(\n",
    "    #     (wte): Embedding(50304, 1024)\n",
    "    #     (drop): Dropout(p=0.0, inplace=False)\n",
    "    #     (h): ModuleList(\n",
    "    #       (0-9): 10 x LlamaBlock(\n",
    "    #         (ln_1): RMSNorm()\n",
    "    #         (attn): LlamaAttention(\n",
    "    #           (c_attn): Linear(in_features=1024, out_features=3072, bias=False)\n",
    "    #           (c_proj): Linear(in_features=1024, out_features=1024, bias=False)\n",
    "    #           (attn_dropout): Dropout(p=0.0, inplace=False)\n",
    "    #           (resid_dropout): Dropout(p=0.0, inplace=False)\n",
    "    #           (rotary_emb): RotaryEmbedding()\n",
    "    #         )\n",
    "    #         (ln_2): RMSNorm()\n",
    "    #         (mlp): LlamaMLP(\n",
    "    #           (w1): Linear(in_features=1024, out_features=2816, bias=False)\n",
    "    #           (w2): Linear(in_features=1024, out_features=2816, bias=False)\n",
    "    #           (c_proj): Linear(in_features=2816, out_features=1024, bias=False)\n",
    "    #         )\n",
    "    #       )\n",
    "    #     )\n",
    "    #     (ln_f): RMSNorm()\n",
    "    #   )\n",
    "    #   (lm_head): Linear(in_features=1024, out_features=50304, bias=False)\n",
    "    #   (rotary_emb): RotaryEmbedding()\n",
    "    # )\n",
    "    # \"\"\"\n",
    "    # if args.peft_type == 'lora':\n",
    "    #     if args.init_lora_weights == 'none':\n",
    "    #         init_lora_weights = True\n",
    "    #     else:\n",
    "    #         init_lora_weights = args.init_lora_weights\n",
    "    #     lora_config = LoraConfig(\n",
    "    #         r=args.lora_r,\n",
    "    #         lora_alpha=args.lora_alpha,\n",
    "    #         init_lora_weights=init_lora_weights,\n",
    "    #         target_modules=[\"c_attn\", \"c_proj\", \"w1\", \"w2\"],\n",
    "    #         lora_dropout=args.lora_dropout\n",
    "    #     )\n",
    "    # elif args.peft_type == 'loha':\n",
    "    #     lora_config = LoHaConfig(\n",
    "    #         r=args.lora_r,\n",
    "    #         lora_alpha=args.lora_alpha,\n",
    "    #         target_modules=[\"c_attn\", \"c_proj\", \"w1\", \"w2\"],\n",
    "    #         lora_dropout=args.lora_dropout\n",
    "    #     )\n",
    "    # elif args.peft_type == 'lokr':\n",
    "    #     lora_config = LoKrConfig(\n",
    "    #         r=args.lora_r,\n",
    "    #         lora_alpha=args.lora_alpha,\n",
    "    #         target_modules=[\"c_attn\", \"c_proj\", \"w1\", \"w2\"],\n",
    "    #         lora_dropout=args.lora_dropout\n",
    "    #     )\n",
    "\n",
    "    # if args.peft_type != 'none':\n",
    "    #     model.config.tie_word_embeddings = model.config.weight_tying # LoRA expects `tie_word_embeddings`\n",
    "    #     if not hasattr(model.config, \"get\"):\n",
    "    #         model.config.get = lambda key, default=None: getattr(model.config, key, default)\n",
    "\n",
    "    #     model = get_peft_model(model, lora_config)\n",
    "    #     model.print_trainable_parameters()\n",
    "\n",
    "    # # Training\n",
    "    # print(f\"\\nTraining model={args.model} \\n{vars(args)}\\n\")\n",
    "    # stats = train(\n",
    "    #     model=model,\n",
    "    #     opt=opt,\n",
    "    #     data=data,\n",
    "    #     data_seed=args.data_seed,\n",
    "    #     scheduler=scheduler,\n",
    "    #     iterations=args.iterations,\n",
    "    #     acc_steps=args.acc_steps,\n",
    "    #     batch_size=args.batch_size,\n",
    "    #     sequence_length=args.sequence_length,\n",
    "    #     eval_freq=args.eval_freq,\n",
    "    #     ckpt_path=f\"{ckpt_path}/ckpt.pt\",\n",
    "    #     distributed_backend=distributed_backend,\n",
    "    #     extra_args=args,\n",
    "    #     itr=itr,\n",
    "    #     rng_state_dict=rng_state_dict,\n",
    "    #     max_duration=args.max_duration\n",
    "    # )\n",
    "\n",
    "    # args.device = None\n",
    "    # args.dtype = None\n",
    "    # stats['args'] = vars(args)\n",
    "    # if distributed_backend.is_master_process():\n",
    "    #     with open(f\"{ckpt_path}/summary.json\", \"w\") as fs:\n",
    "    #         json.dump(stats, fs)\n",
    "    # distributed_backend.finalize()\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.argv = [\n",
    "    \"notebook\",  # Dummy program name\n",
    "    \"--config\", CFG_PATH,\n",
    "]\n",
    "\n",
    "from main import get_args  # Adjust this import path as needed\n",
    "\n",
    "args = get_args()\n",
    "main(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f18f0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
