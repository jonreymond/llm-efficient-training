model: "noam"
weight_tying: True

compile: True
grad_clip: 1.0
save_checkpoint_freq: 500
wandb_project: "lauzhack-llm"
wandb_group: "final"
run_prefix: "mathqa_full"
wandb: True
n_layer: 10
n_embd: 1024
n_head: 16
opt: "adamw"
scheduler: "cos"
weight_decay: 0.1

lr: 0.000005
#rho: 0.05

iterations: 4000 ## train itr: 1200 ms, 3 hours of training => num iterations = (1/1.23)*3*60*60 = 8780
batch_size: 64
acc_steps: 4

eval_freq: 500
eval_seq_prefix: "none"



use_pretrained: "./exps/slimpajama/noam/better_scheduler_noam_lr0.0008_bs64x4_seqlen512/iterations=10000_scheduler=cycle_eval_freq=2000_grad_clip=1.0_weight_tying=True_n_head=16_n_layer=10_n_embd=1024_compile=True_save_checkpoint_freq=500_seed=0/ckpt.pt" # "auto"


max_duration: 3600 # {1h: 3600, 2h: 7200, 3h: 10800, 4h: 14400}
peft_type: "none"

dataset: "mathqa" # [slimpajama, mathqa]

qat: False